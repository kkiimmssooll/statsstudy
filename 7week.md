# ch6. 통계적 머신러닝

| 목차 | 내용                   |
|------|------------------------|
| 6.1  | K 최근접 이웃          |
| 6.2  | 트리 모델              |
| 6.3  | 배깅과 랜덤 포레스트   |
| 6.4  | 부스팅                 |

## 6.1 K 최근접 이웃(KNN)
> 비슷한 레코드들이 어떻게 분류되는지에 따라 해당 레코드를 분류
1. 특징이 가장 유사한 K개의 레코드를 찾고
2. (분류) 이 레코드들 중 다수가 속한 클래스를 찾은 후 새로운 레코드를 그 클래스에 할당
3. (예측) 유사한 레코드들의 평균을 찾아 새로운 레코드의 예측값으로 사용

### 거리 지표
> 유사성을 결정하는 기준
1. 유클리드 거리 -> 서로의 차이에 대한 제곱합을 구한 뒤 그 값의 제곱근을 취함 -> 두 점 사이의 직선 거리
2. 맨하탄 거리 -> 대각선이 아닌 축 방향으로만 움직일 수 있다고 할 때 두 점 사이의 거리

### K 선택
```
K=1은 새로운 데이터랑 가장 가까운 데이터를 예측 결과로 쓰는 것
-> K값이 너무 작으면 노이즈 성분까지 고려하는 오버피팅 문제 발생
-> K값이 너무 크면 너무 많은 데이터를 참고하니까 소수의 중요한 특징이 무시됨
```
✅ 그럼 어떻게 결정??
-> 데이터에 따라 다름!
-> 노이즈가 거의 없고 구조화되면 K값이 작을수록 좋고
-> 노이즈가 많으면 K값이 클수록 좋음

## 6.3 배깅과 랜덤 포레스트
> 배깅 : 데이터를 부트스트래핑해서 여러 모델을 만드는 일반적인 방법(Bootstrap Aggregating)
```
1. 데이터를 여러 개로 나눔
2. 원래 학습 데이터를 중복 허용하면서 랜덤하게 뽑음 (부트스트랩 샘플링) → 예: 전체 데이터에서 중복 허용해서 100개 뽑기
3. 각 데이터로 모델을 따로 학습시킴
    - 모델은 보통 의사결정나무(Decision Tree)를 씀
4. 예측
    - 분류 문제: 각 모델의 투표 결과로 다수결
    - 회귀 문제: 각 모델의 예측값 평균
```

> 랜덤 포레스트 : 레코드를 표본추출할 때 변수 샘플링이 추가된 배깅
```
1. 여러 개의 부트스트랩 샘플 생성→ 전체 데이터에서 중복 허용해서 랜덤하게 샘플링
2. 각 샘플로 결정 트리(Decision Tree) 학습
    - 📌 여기서 배깅과의 핵심적인 차이이
        - 노드 분할할 때, 전체 특성 중 일부만 랜덤으로 선택해서 사용함
            예: 총 10개의 특성이 있어도, 매 노드마다 그중 랜덤하게 3개만 보고 최적 분할 결정
3. 예측
    - 분류: 트리들 다수결 투표
    - 회귀: 트리들 평균
```

| 항목       | 배깅 (Bagging)                               | 랜덤 포레스트 (Random Forest)          |
| -------- | ------------------------------------------ | -------------------------------- |
| 특성 선택 방식 | ❌ 모든 특성을 사용해 트리 분기 결정                      | 각 노드마다 일부 특성만 랜덤 선택해 분기 결정 |
| 모델 다양성   | 낮음 (트리들이 비슷하게 생김)                          | 높음 (트리들이 각자 다른 특성 기반으로 자람)       |
| 과적합 위험   | 있음 (특히 강한 예측력이 있는 특성이 있을 경우 모든 트리가 그걸 따라감) | 더 낮음 (랜덤하게 특성을 골라서 편향 줄임)        |
| 대표 모델    | 단순한 결정 트리 앙상블                              | 랜덤한 특성 + 트리 앙상블 → 랜덤 포레스트        |

## 6.4 부스팅
> 이전 모델이 갖는 오차를 줄이는 방향으로 다음 모델 연속적으로 생성
> 각 모델이 조금씩만 잘하는 단순 모델이라도, 잘 결합하면 매우 강력한 하나의 모델이 될 수 있음
> 약한 모델을 모아서 강한 모델을 만드는 개념!!
```
1. 처음에는 모든 데이터에 동일한 가중치를 줌
2. 첫 번째 모델을 학습하고, 예측해봄
3. 틀린 데이터에 더 높은 가중치를 줌 → 다음 모델은 이 틀린 데이터를 더 집중적으로 학습함
4. 1~3 과정을 반복함 (모델 → 모델 → 모델…)
5. 마지막에 모든 모델의 예측을 가중합해서 최종 예측
```

| 알고리즘              | 특징                                         |
| ----------------- | ------------------------------------------ |
| AdaBoost          | 틀린 데이터에 가중치를 부여해 보완                        |
| Gradient Boosting | 오차(잔차)를 줄이는 방향으로 다음 모델을 학습                 |
| XGBoost           | Gradient Boosting의 속도와 성능을 개선한 버전 (실무에서 많이 사용) |
| LightGBM          | 대용량 데이터에 특화된 Gradient Boosting             |

```
파라미터
1. subsample : 각 반복 구간마다 샘플링할 입력 데이터의 비율 조정
2. eta : 축소 비율 결정->가중치의 변화량을 낮춰 오버피팅 방지
```

### 배깅 vs. 부스팅
| 항목      | 배깅 (Bagging)        | 부스팅 (Boosting)                         |
| ------- | ------------------- | -------------------------------------- |
| 모델 학습   | 병렬로 독립적으로 학습    | 순차적으로 연결 학습                        |
| 목적      | 분산(variance) 감소 | 편향(bias) 감소                        |
| 오류 처리   | 각 모델이 동등한 역할    | 이전 모델의 오류를 다음 모델이 보완               |
| 대표 알고리즘 | 랜덤 포레스트             | AdaBoost, Gradient Boosting, XGBoost 등 |


# ch7. 비지도학습

| 목차 | 내용                   |
|------|------------------------|
| 7.1  | 주성분분석             |
| 7.2  | K 평균 클러스터링      |
| 7.3  | 계층적 클러스터링      |
| 7.4  | 모델 기반 클러스터링   |
| 7.5  | 스케일링과 범주형 변수 |

## 7.1 주성분분석(PCA)
> 수치형 변수들이 어떤 식으로 공변하는지 알아내는 기법
```
1. 평균 0으로 정규화 (중심 이동)
2. 공분산 행렬 계산
    - 어떤 특성들이 함께 변하는지 측정
3. 고유값/고유벡터 계산 (eigen decomposition)
    - 고유벡터 = 새로운 축 (주성분)
    - 고유값 = 축의 중요도 (분산 크기)
4. 중요한 축 몇 개만 선택
    - 예: 전체 분산의 95%를 설명하는 축 2개만 선택
5. 원본 데이터를 그 축들로 다시 표현 → 차원 축소
```

### 스크리그래프
**주성분의 상대적인 중요도를 표시해줌**
> 각 주성분이 설명하는 분산의 비율을 시각화한 그래프 → 어떤 주성분까지 선택할지를 결정하는 데 도움을 줌

![study_1](/stats_study/7.1.png)
✅ 엘보우 포인트 활용용

## 7.5 스케일링과 범주형 변수
### 표준화
> 평균을 빼고 표준편차로 나눠서 원래 값을 z 점수로 변환

> 변수 스케일링을 하지 않으면 큰 값의 변수들이 클러스터링 결과를 좌우하게 됨

### 지배 변수
> 스케일이 너무 커서 전체 분석 결과에 지나치게 큰 영향을 미치는 변수

> 변수를 스케일링하거나 지배 변수는 전체 분석에서 제외

### 고워 거리
> 범주형 변수와 수치형 변수가 섞여 있을 때 사용할 수 있는 혼합형 거리 측정 방법

> 범주형 데이터가 있는 경우 순서형, 이진형 변수를 사용하여 수치형 데이터로 변환해야 함-> 각 변수의 데이터 유형에 따라 거리 지표를 다르게 적용

💡예를 들면
| 나이 (수치형) | 성별 (범주형) |
| -------- | -------- |
| A: 30    | 남        |
| B: 60    | 여        |
![study_2](/stats_study/7.5.png)